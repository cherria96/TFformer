GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.
  rank_zero_warn(
  | Name                | Type               | Params
-----------------------------------------------------------
0 | embedding_layer     | Linear             | 20
1 | positional_encoding | PositionalEncoding | 0
2 | transformer_encoder | ModuleList         | 1.1 K
3 | predict_outcomes    | Sequential         | 154
4 | output_dropout      | Dropout            | 0
5 | loss_fn             | MSELoss            | 0
-----------------------------------------------------------
1.3 K     Trainable params
0         Non-trainable params
1.3 K     Total params
0.005     Total estimated model params size (MB)
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Sanity Checking DataLoader 0:   0%|                                                                        | 0/1 [00:00<?, ?it/s]
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=40). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.


Epoch 0:   0%|                                                                                             | 0/4 [00:00<?, ?it/s]mse_loss tensor(1.1624, grad_fn=<MseLossBackward0>)

Epoch 0:  25%|███████████████▌                                              | 1/4 [00:49<02:29, 49.89s/it, loss=1.16, v_num=bpkj]mse_loss tensor(1.0776, grad_fn=<MseLossBackward0>)

Epoch 0:  50%|███████████████████████████████                               | 2/4 [01:45<01:45, 52.57s/it, loss=1.12, v_num=bpkj]mse_loss tensor(1.0396, grad_fn=<MseLossBackward0>)
Epoch 0:  75%|██████████████████████████████████████████████▌               | 3/4 [02:27<00:49, 49.19s/it, loss=1.09, v_num=bpkj]


Epoch 1:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.09, v_num=bpkj]mse_loss tensor(1.0186, grad_fn=<MseLossBackward0>)

Epoch 1:  25%|███████████████▌                                              | 1/4 [00:54<02:43, 54.37s/it, loss=1.07, v_num=bpkj]mse_loss tensor(1.0224, grad_fn=<MseLossBackward0>)

Epoch 1:  50%|███████████████████████████████                               | 2/4 [01:41<01:41, 50.90s/it, loss=1.06, v_num=bpkj]mse_loss tensor(1.0134, grad_fn=<MseLossBackward0>)
Epoch 1:  75%|██████████████████████████████████████████████▌               | 3/4 [02:23<00:47, 47.77s/it, loss=1.06, v_num=bpkj]



Epoch 2:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.06, v_num=bpkj]mse_loss tensor(1.0156, grad_fn=<MseLossBackward0>)

Epoch 2:  25%|███████████████▌                                              | 1/4 [00:50<02:31, 50.45s/it, loss=1.05, v_num=bpkj]mse_loss tensor(1.0177, grad_fn=<MseLossBackward0>)

Epoch 2:  50%|███████████████████████████████                               | 2/4 [01:36<01:36, 48.02s/it, loss=1.05, v_num=bpkj]mse_loss tensor(1.0109, grad_fn=<MseLossBackward0>)
Epoch 2:  75%|██████████████████████████████████████████████▌               | 3/4 [02:14<00:44, 44.95s/it, loss=1.04, v_num=bpkj]



Epoch 3:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.04, v_num=bpkj]mse_loss tensor(1.0034, grad_fn=<MseLossBackward0>)

Epoch 3:  25%|███████████████▌                                              | 1/4 [00:56<02:48, 56.17s/it, loss=1.04, v_num=bpkj]mse_loss tensor(0.9978, grad_fn=<MseLossBackward0>)

Epoch 3:  50%|███████████████████████████████                               | 2/4 [01:43<01:43, 51.68s/it, loss=1.03, v_num=bpkj]mse_loss tensor(1.0153, grad_fn=<MseLossBackward0>)
Epoch 3:  75%|██████████████████████████████████████████████▌               | 3/4 [02:29<00:49, 49.87s/it, loss=1.03, v_num=bpkj]


Epoch 4:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.03, v_num=bpkj]mse_loss tensor(1.0083, grad_fn=<MseLossBackward0>)

Epoch 4:  25%|███████████████▌                                              | 1/4 [00:48<02:24, 48.30s/it, loss=1.03, v_num=bpkj]mse_loss tensor(1.0048, grad_fn=<MseLossBackward0>)

Epoch 4:  50%|███████████████████████████████                               | 2/4 [01:43<01:43, 51.54s/it, loss=1.03, v_num=bpkj]mse_loss tensor(0.9859, grad_fn=<MseLossBackward0>)
Epoch 4:  75%|██████████████████████████████████████████████▌               | 3/4 [02:32<00:50, 50.71s/it, loss=1.03, v_num=bpkj]


Epoch 5:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.03, v_num=bpkj]mse_loss tensor(1.0074, grad_fn=<MseLossBackward0>)

Epoch 5:  25%|███████████████▌                                              | 1/4 [00:53<02:41, 53.69s/it, loss=1.03, v_num=bpkj]mse_loss tensor(1.0009, grad_fn=<MseLossBackward0>)

Epoch 5:  50%|███████████████████████████████                               | 2/4 [01:37<01:37, 48.79s/it, loss=1.02, v_num=bpkj]mse_loss tensor(0.9889, grad_fn=<MseLossBackward0>)
Epoch 5:  75%|██████████████████████████████████████████████▌               | 3/4 [02:16<00:45, 45.38s/it, loss=1.02, v_num=bpkj]


Epoch 6:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.02, v_num=bpkj]mse_loss tensor(0.9908, grad_fn=<MseLossBackward0>)
Epoch 6:  25%|███████████████▌                                              | 1/4 [01:02<03:08, 62.83s/it, loss=1.02, v_num=bpkj]
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.

Testing DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.64s/it]
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Runningstage.testing metric      DataLoader 0
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_mse_loss         1.0140638796834085
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────