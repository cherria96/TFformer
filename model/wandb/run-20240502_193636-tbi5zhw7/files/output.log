[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name                | Type                | Params
------------------------------------------------------------
0 | decomposition       | SeriesDecomposition | 0
1 | embedding_layer     | Linear              | 64
2 | positional_encoding | PositionalEncoding  | 0
3 | transformer_encoder | ModuleList          | 19.4 K
4 | linear1             | Sequential          | 545
5 | linear2             | Sequential          | 4.2 K
6 | output_dropout      | Dropout             | 0
7 | loss_fn             | MSELoss             | 0
------------------------------------------------------------
24.2 K    Trainable params
0         Non-trainable params
24.2 K    Total params
0.097     Total estimated model params size (MB)
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

Sanity Checking DataLoader 0:  50%|████████▌        | 1/2 [00:12<00:12, 12.27s/it]
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=40). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.


















Epoch 0:  72%|▋| 18/25 [05:
























Epoch 1:  72%|▋| 18/25 [05:
























Epoch 2:  72%|▋| 18/25 [03:11<01:14, 10.64s/it, loss=0.727, v_num=zh























Epoch 3:  72%|▋| 18/25 [02:17<00:53,  7.67s/it, loss=0.577, v_num=zh
























Epoch 4:  72%|▋| 18/25 [02:03<00:48,  6.88s/it, loss=0.464, v_num=zh
























Epoch 5:  72%|▋| 18/25 [02:05<00:48,  6.95s/it, loss=0.379, v_num=zh
























Epoch 6:  72%|▋| 18/25 [02:05<00:48,  6.97s/it, loss=0.335, v_num=zh























Epoch 7:  72%|▋| 18/25 [02:15<00:52,  7.51s/it, loss=0.307, v_num=zh























Epoch 8:  72%|▋| 18/25 [02:08<00:49,  7.13s/it, loss=0.29, v_num=zhw























Epoch 9:  72%|▋| 18/25 [02:06<00:49,  7.03s/it, loss=0.281, v_num=zh
























Epoch 10:  72%|▋| 18/25 [02:33<00:59,  8.50s/it, loss=0.277, v_num=z























Epoch 11:  72%|▋| 18/25 [02:35<01:00,  8.62s/it, loss=0.264, v_num=z
























Epoch 12:  72%|▋| 18/25 [02:40<01:02,  8.92s/it, loss=0.258, v_num=z























Epoch 13:  72%|▋| 18/25 [02:27<00:57,  8.18s/it, loss=0.252, v_num=z
























Epoch 14:  72%|▋| 18/25 [02:17<00:53,  7.67s/it, loss=0.246, v_num=z























Epoch 15:  72%|▋| 18/25 [02:09<00:50,  7.18s/it, loss=0.246, v_num=z























Epoch 16:  72%|▋| 18/25 [02:03<00:47,  6.83s/it, loss=0.242, v_num=z























Epoch 17:  72%|▋| 18/25 [02:02<00:47,  6.81s/it, loss=0.237, v_num=z
























Epoch 18:  72%|▋| 18/25 [02:06<00:49,  7.02s/it, loss=0.235, v_num=z























Epoch 19:  72%|▋| 18/25 [02:11<00:51,  7.32s/it, loss=0.235, v_num=z
























Epoch 20:  72%|▋| 18/25 [02:26<00:56,  8.13s/it, loss=0.231, v_num=z























Epoch 21:  72%|▋| 18/25 [02:02<00:47,  6.80s/it, loss=0.232, v_num=z
























Epoch 22:  72%|▋| 18/25 [02:01<00:47,  6.75s/it, loss=0.235, v_num=z
























Epoch 23:  72%|▋| 18/25 [02:13<00:52,  7.43s/it, loss=0.231, v_num=z
























Epoch 24:  72%|▋| 18/25 [02:00<00:46,  6.71s/it, loss=0.229, v_num=z
























Epoch 25:  72%|██████▍  | 18/25 [02:04<00:48,  6.93s/it, loss=0.231, v_num=zhw7]























Epoch 26:  72%|███████▏  | 18/25 [02:09<00:50,  7.19s/it, loss=0.23, v_num=zhw7]























Epoch 27:  72%|██████▍  | 18/25 [02:00<00:46,  6.71s/it, loss=0.227, v_num=zhw7]






Epoch 30:  76%|██████▊  | 19/25 [02:09<00:40,  6.79s/it, loss=0.229, v_num=zhw7]























Epoch 31:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.227, v_num=zhw7]























Epoch 32:  72%|██████▍  | 18/25 [02:04<00:48,  6.92s/it, loss=0.228, v_num=zhw7]























Epoch 33:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.227, v_num=zhw7]























Epoch 34:  72%|██████▍  | 18/25 [02:01<00:47,  6.73s/it, loss=0.223, v_num=zhw7]























Epoch 35:  72%|██████▍  | 18/25 [01:59<00:46,  6.63s/it, loss=0.225, v_num=zhw7]























Epoch 36:  72%|██████▍  | 18/25 [02:01<00:47,  6.78s/it, loss=0.228, v_num=zhw7]
























Epoch 37:  72%|██████▍  | 18/25 [02:01<00:47,  6.73s/it, loss=0.224, v_num=zhw7]























Epoch 38:  72%|██████▍  | 18/25 [02:01<00:47,  6.73s/it, loss=0.221, v_num=zhw7]























Epoch 39:  72%|██████▍  | 18/25 [02:03<00:48,  6.87s/it, loss=0.224, v_num=zhw7]
























Epoch 40:  72%|██████▍  | 18/25 [02:02<00:47,  6.83s/it, loss=0.225, v_num=zhw7]























Epoch 41:  72%|██████▍  | 18/25 [02:01<00:47,  6.78s/it, loss=0.226, v_num=zhw7]
























Epoch 42:  72%|██████▍  | 18/25 [02:00<00:46,  6.71s/it, loss=0.224, v_num=zhw7]























Epoch 43:  72%|██████▍  | 18/25 [02:00<00:46,  6.70s/it, loss=0.222, v_num=zhw7]























Epoch 44:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.221, v_num=zhw7]
























Epoch 45:  72%|██████▍  | 18/25 [02:01<00:47,  6.74s/it, loss=0.223, v_num=zhw7]























Epoch 46:  72%|██████▍  | 18/25 [02:01<00:47,  6.75s/it, loss=0.223, v_num=zhw7]























Epoch 47:  72%|███████▏  | 18/25 [02:00<00:46,  6.70s/it, loss=0.22, v_num=zhw7]























Epoch 48:  72%|██████▍  | 18/25 [01:59<00:46,  6.67s/it, loss=0.219, v_num=zhw7]























Epoch 49:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.222, v_num=zhw7]























Epoch 50:  72%|██████▍  | 18/25 [02:02<00:47,  6.78s/it, loss=0.221, v_num=zhw7]























Epoch 51:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.219, v_num=zhw7]























Epoch 52:  72%|██████▍  | 18/25 [02:00<00:46,  6.70s/it, loss=0.216, v_num=zhw7]
























Epoch 53:  72%|██████▍  | 18/25 [02:01<00:47,  6.73s/it, loss=0.219, v_num=zhw7]























Epoch 54:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.219, v_num=zhw7]























Epoch 55:  72%|██████▍  | 18/25 [02:00<00:47,  6.72s/it, loss=0.219, v_num=zhw7]























Epoch 56:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.218, v_num=zhw7]























Epoch 57:  72%|██████▍  | 18/25 [02:02<00:47,  6.79s/it, loss=0.218, v_num=zhw7]























Epoch 58:  72%|██████▍  | 18/25 [02:02<00:47,  6.78s/it, loss=0.218, v_num=zhw7]























Epoch 59:  72%|██████▍  | 18/25 [02:01<00:47,  6.75s/it, loss=0.221, v_num=zhw7]























Epoch 60:  72%|██████▍  | 18/25 [02:00<00:46,  6.70s/it, loss=0.217, v_num=zhw7]























Epoch 61:  72%|██████▍  | 18/25 [02:01<00:47,  6.73s/it, loss=0.217, v_num=zhw7]























Epoch 62:  72%|██████▍  | 18/25 [02:01<00:47,  6.75s/it, loss=0.219, v_num=zhw7]























Epoch 63:  72%|██████▍  | 18/25 [02:01<00:47,  6.72s/it, loss=0.215, v_num=zhw7]























Epoch 64:  72%|██████▍  | 18/25 [02:00<00:46,  6.70s/it, loss=0.215, v_num=zhw7]























Epoch 65:  72%|██████▍  | 18/25 [02:05<00:48,  6.95s/it, loss=0.214, v_num=zhw7]























Epoch 66:  72%|██████▍  | 18/25 [01:59<00:46,  6.65s/it, loss=0.216, v_num=zhw7]























Epoch 67:  72%|██████▍  | 18/25 [02:00<00:46,  6.68s/it, loss=0.215, v_num=zhw7]
























Epoch 68:  72%|██████▍  | 18/25 [02:00<00:46,  6.70s/it, loss=0.218, v_num=zhw7]

























Epoch 69:  72%|██████▍  | 18/25 [02:02<00:47,  6.79s/it, loss=0.217, v_num=zhw7]
























Epoch 70:  72%|██████▍  | 18/25 [02:02<00:47,  6.79s/it, loss=0.215, v_num=zhw7]























Epoch 71:  72%|██████▍  | 18/25 [02:04<00:48,  6.93s/it, loss=0.214, v_num=zhw7]























Epoch 72:  72%|██████▍  | 18/25 [02:01<00:47,  6.75s/it, loss=0.215, v_num=zhw7]























Epoch 73:  72%|██████▍  | 18/25 [02:02<00:47,  6.80s/it, loss=0.217, v_num=zhw7]























Epoch 74:  72%|██████▍  | 18/25 [02:01<00:47,  6.74s/it, loss=0.216, v_num=zhw7]
























Epoch 75:  72%|██████▍  | 18/25 [02:01<00:47,  6.72s/it, loss=0.216, v_num=zhw7]























Epoch 76:  72%|██████▍  | 18/25 [02:01<00:47,  6.76s/it, loss=0.213, v_num=zhw7]

























Epoch 77:  72%|██████▍  | 18/25 [02:03<00:47,  6.85s/it, loss=0.211, v_num=zhw7]























Epoch 78:  72%|██████▍  | 18/25 [02:02<00:47,  6.80s/it, loss=0.214, v_num=zhw7]























Epoch 79:  72%|██████▍  | 18/25 [02:02<00:47,  6.81s/it, loss=0.214, v_num=zhw7]























Epoch 80:  72%|██████▍  | 18/25 [02:02<00:47,  6.82s/it, loss=0.212, v_num=zhw7]























Epoch 81:  72%|██████▍  | 18/25 [02:02<00:47,  6.79s/it, loss=0.212, v_num=zhw7]























Epoch 82:  72%|██████▍  | 18/25 [02:00<00:46,  6.69s/it, loss=0.214, v_num=zhw7]























Epoch 83:  72%|██████▍  | 18/25 [02:00<00:46,  6.70s/it, loss=0.214, v_num=zhw7]























Epoch 84:  72%|██████▍  | 18/25 [02:00<00:46,  6.68s/it, loss=0.214, v_num=zhw7]
























Epoch 85:  72%|██████▍  | 18/25 [02:01<00:47,  6.73s/it, loss=0.213, v_num=zhw7]























Epoch 86:  72%|██████▍  | 18/25 [02:01<00:47,  6.75s/it, loss=0.213, v_num=zhw7]
























Epoch 87:  72%|██████▍  | 18/25 [02:02<00:47,  6.80s/it, loss=0.213, v_num=zhw7]























Epoch 88:  72%|██████▍  | 18/25 [02:03<00:48,  6.88s/it, loss=0.214, v_num=zhw7]























Epoch 89:  72%|██████▍  | 18/25 [02:02<00:47,  6.79s/it, loss=0.213, v_num=zhw7]























Epoch 90:  72%|███████▏  | 18/25 [02:02<00:47,  6.78s/it, loss=0.21, v_num=zhw7]























Epoch 91:  72%|███████▏  | 18/25 [02:01<00:47,  6.75s/it, loss=0.21, v_num=zhw7]























Epoch 92:  72%|██████▍  | 18/25 [02:01<00:47,  6.74s/it, loss=0.209, v_num=zhw7]























Epoch 93:  72%|██████▍  | 18/25 [02:03<00:47,  6.84s/it, loss=0.211, v_num=zhw7]























Epoch 94:  72%|██████▍  | 18/25 [02:04<00:48,  6.92s/it, loss=0.209, v_num=zhw7]























Epoch 95:  72%|███████▏  | 18/25 [02:03<00:47,  6.86s/it, loss=0.21, v_num=zhw7]























Epoch 96:  72%|██████▍  | 18/25 [02:04<00:48,  6.90s/it, loss=0.213, v_num=zhw7]























Epoch 97:  72%|███████▏  | 18/25 [02:03<00:48,  6.88s/it, loss=0.21, v_num=zhw7]























Epoch 98:  72%|███████▏  | 18/25 [02:01<00:47,  6.76s/it, loss=0.21, v_num=zhw7]























Epoch 99:  72%|██████▍  | 18/25 [02:01<00:47,  6.78s/it, loss=0.208, v_num=zhw7]






Testing DataLoader 0:   0%|                              | 0/10 [00:00<?, ?it/s]
`Trainer.fit` stopped: `max_epochs=100` reached.
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.









Testing DataLoader 0: 100%|█████████████████████| 10/10 [00:59<00:00,  5.98s/it]
────────────────────────────────────────────────────────────────────────────────
Runningstage.testing metric      DataLoader 0
────────────────────────────────────────────────────────────────────────────────
      test_mse_loss         0.16930825239309288
────────────────────────────────────────────────────────────────────────────────