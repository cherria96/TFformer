[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name                | Type                | Params
------------------------------------------------------------
0 | decomposition       | SeriesDecomposition | 0
1 | embedding_layer     | Linear              | 64
2 | positional_encoding | PositionalEncoding  | 0
3 | transformer_encoder | ModuleList          | 19.4 K
4 | linear1             | Sequential          | 545
5 | linear2             | Sequential          | 4.2 K
6 | output_dropout      | Dropout             | 0
7 | loss_fn             | MSELoss             | 0
------------------------------------------------------------
24.2 K    Trainable params
0         Non-trainable params
24.2 K    Total params
0.097     Total estimated model params size (MB)
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

Sanity Checking DataLoader 0:  50%|██████▌      | 1/2 [00:12<00:12, 12.10s/it]
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/user126/anaconda3/envs/dt_idl/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=40). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.


















Epoch 0:  72%|██████▍  | 18/25 [05:36<02:10, 18.69s/it, loss=1.29, v_num=gvk2]
























Epoch 1:  72%|█████▊  | 18/25 [02:09<00:50,  7.21s/it, loss=0.817, v_num=gvk2]























Epoch 2:  72%|█████▊  | 18/25 [01:49<00:42,  6.08s/it, loss=0.608, v_num=gvk2]























Epoch 3:  72%|█████▊  | 18/25 [01:51<00:43,  6.17s/it, loss=0.497, v_num=gvk2]























Epoch 4:  72%|██████▍  | 18/25 [01:51<00:43,  6.18s/it, loss=0.41, v_num=gvk2]























Epoch 5:  72%|█████▊  | 18/25 [01:52<00:43,  6.25s/it, loss=0.362, v_num=gvk2]























Epoch 6:  72%|█████▊  | 18/25 [01:50<00:42,  6.13s/it, loss=0.327, v_num=gvk2]























Epoch 7:  72%|█████▊  | 18/25 [01:47<00:41,  5.98s/it, loss=0.308, v_num=gvk2]






















Epoch 8:  72%|█████▊  | 18/25 [01:49<00:42,  6.06s/it, loss=0.295, v_num=gvk2]
























Epoch 9:  72%|█████▊  | 18/25 [01:49<00:42,  6.08s/it, loss=0.279, v_num=gvk2]























Epoch 10:  72%|█████  | 18/25 [01:51<00:43,  6.20s/it, loss=0.267, v_num=gvk2]























Epoch 11:  72%|█████  | 18/25 [01:50<00:42,  6.11s/it, loss=0.266, v_num=gvk2]























Epoch 12:  72%|█████  | 18/25 [01:51<00:43,  6.17s/it, loss=0.257, v_num=gvk2]























Epoch 13:  72%|█████  | 18/25 [01:51<00:43,  6.20s/it, loss=0.251, v_num=gvk2]























Epoch 14:  72%|█████  | 18/25 [01:51<00:43,  6.18s/it, loss=0.247, v_num=gvk2]























Epoch 15:  72%|█████  | 18/25 [01:49<00:42,  6.11s/it, loss=0.243, v_num=gvk2]























Epoch 16:  72%|█████  | 18/25 [01:50<00:42,  6.13s/it, loss=0.239, v_num=gvk2]























Epoch 17:  72%|█████  | 18/25 [01:51<00:43,  6.20s/it, loss=0.239, v_num=gvk2]























Epoch 18:  72%|█████  | 18/25 [01:51<00:43,  6.19s/it, loss=0.239, v_num=gvk2]
























Epoch 19:  72%|█████  | 18/25 [01:52<00:43,  6.23s/it, loss=0.237, v_num=gvk2]























Epoch 20:  72%|█████  | 18/25 [01:54<00:44,  6.38s/it, loss=0.237, v_num=gvk2]























Epoch 21:  72%|█████  | 18/25 [01:52<00:43,  6.25s/it, loss=0.234, v_num=gvk2]























Epoch 22:  72%|█████  | 18/25 [01:53<00:44,  6.32s/it, loss=0.232, v_num=gvk2]























Epoch 23:  72%|█████  | 18/25 [01:54<00:44,  6.34s/it, loss=0.232, v_num=gvk2]























Epoch 24:  72%|█████▊  | 18/25 [01:48<00:42,  6.05s/it, loss=0.23, v_num=gvk2]
























Epoch 25:  72%|█████  | 18/25 [01:49<00:42,  6.09s/it, loss=0.233, v_num=gvk2]























Epoch 26:  72%|█████  | 18/25 [01:50<00:42,  6.11s/it, loss=0.233, v_num=gvk2]























Epoch 27:  72%|█████▊  | 18/25 [01:51<00:43,  6.18s/it, loss=0.23, v_num=gvk2]























Epoch 28:  72%|█████  | 18/25 [01:51<00:43,  6.18s/it, loss=0.229, v_num=gvk2]























Epoch 29:  72%|█████  | 18/25 [01:52<00:43,  6.23s/it, loss=0.229, v_num=gvk2]























Epoch 30:  72%|█████  | 18/25 [01:52<00:43,  6.23s/it, loss=0.226, v_num=gvk2]























Epoch 31:  72%|█████  | 18/25 [01:52<00:43,  6.23s/it, loss=0.227, v_num=gvk2]























Epoch 32:  72%|█████  | 18/25 [01:50<00:43,  6.17s/it, loss=0.229, v_num=gvk2]
























Epoch 33:  72%|█████  | 18/25 [01:48<00:42,  6.04s/it, loss=0.225, v_num=gvk2]
























Epoch 34:  72%|█████  | 18/25 [01:48<00:42,  6.02s/it, loss=0.228, v_num=gvk2]























Epoch 35:  72%|█████  | 18/25 [01:49<00:42,  6.09s/it, loss=0.227, v_num=gvk2]























Epoch 36:  72%|█████  | 18/25 [01:56<00:45,  6.49s/it, loss=0.224, v_num=gvk2]























Epoch 37:  72%|█████  | 18/25 [01:53<00:44,  6.30s/it, loss=0.225, v_num=gvk2]























Epoch 38:  72%|█████  | 18/25 [01:53<00:43,  6.28s/it, loss=0.226, v_num=gvk2]























Epoch 39:  72%|█████  | 18/25 [01:53<00:44,  6.31s/it, loss=0.222, v_num=gvk2]























Epoch 40:  72%|█████  | 18/25 [01:53<00:44,  6.33s/it, loss=0.225, v_num=gvk2]























Epoch 41:  72%|█████  | 18/25 [02:00<00:46,  6.71s/it, loss=0.224, v_num=gvk2]























Epoch 42:  72%|█████  | 18/25 [01:52<00:43,  6.25s/it, loss=0.225, v_num=gvk2]























Epoch 43:  72%|█████  | 18/25 [02:00<00:46,  6.68s/it, loss=0.223, v_num=gvk2]























Epoch 44:  72%|█████  | 18/25 [02:03<00:47,  6.84s/it, loss=0.224, v_num=gvk2]























Epoch 45:  72%|█████  | 18/25 [02:03<00:48,  6.88s/it, loss=0.221, v_num=gvk2]























Epoch 46:  72%|█████▊  | 18/25 [02:02<00:47,  6.78s/it, loss=0.22, v_num=gvk2]























Epoch 47:  72%|█████  | 18/25 [02:02<00:47,  6.83s/it, loss=0.223, v_num=gvk2]
























Epoch 48:  72%|█████  | 18/25 [02:03<00:48,  6.86s/it, loss=0.219, v_num=gvk2]























Epoch 49:  72%|█████  | 18/25 [02:03<00:48,  6.86s/it, loss=0.218, v_num=gvk2]























Epoch 50:  72%|█████  | 18/25 [01:53<00:44,  6.31s/it, loss=0.219, v_num=gvk2]
























Epoch 51:  72%|█████▊  | 18/25 [01:54<00:44,  6.35s/it, loss=0.22, v_num=gvk2]























Epoch 52:  72%|█████  | 18/25 [01:55<00:44,  6.40s/it, loss=0.218, v_num=gvk2]























Epoch 53:  72%|█████  | 18/25 [01:55<00:44,  6.40s/it, loss=0.216, v_num=gvk2]























Epoch 54:  72%|█████  | 18/25 [01:57<00:45,  6.53s/it, loss=0.216, v_num=gvk2]























Epoch 55:  72%|█████  | 18/25 [01:58<00:46,  6.61s/it, loss=0.217, v_num=gvk2]











