GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.
  rank_zero_warn(
  | Name                | Type               | Params
-----------------------------------------------------------
0 | embedding_layer     | Linear             | 20
1 | positional_encoding | PositionalEncoding | 0
2 | transformer_encoder | ModuleList         | 1.1 K
3 | predict_outcomes    | Sequential         | 121
4 | output_dropout      | Dropout            | 0
5 | loss_fn             | MSELoss            | 0
-----------------------------------------------------------
1.3 K     Trainable params
0         Non-trainable params
1.3 K     Total params
0.005     Total estimated model params size (MB)
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

Epoch 0:   0%|                                                                                             | 0/4 [00:00<?, ?it/s]
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=40). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

Epoch 0:   0%|                                                                                             | 0/4 [00:00<?, ?it/s]mse_loss tensor(1.0801, grad_fn=<MseLossBackward0>)

Epoch 0:  25%|███████████████▌                                              | 1/4 [00:36<01:50, 36.96s/it, loss=1.08, v_num=noih]mse_loss tensor(1.0045, grad_fn=<MseLossBackward0>)

Epoch 0:  50%|███████████████████████████████                               | 2/4 [01:07<01:07, 33.83s/it, loss=1.04, v_num=noih]mse_loss tensor(1.0403, grad_fn=<MseLossBackward0>)
Epoch 0:  75%|██████████████████████████████████████████████▌               | 3/4 [01:27<00:29, 29.21s/it, loss=1.04, v_num=noih]


Epoch 1:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.04, v_num=noih]mse_loss tensor(1.0225, grad_fn=<MseLossBackward0>)

Epoch 1:  25%|███████████████▌                                              | 1/4 [00:28<01:26, 28.97s/it, loss=1.04, v_num=noih]mse_loss tensor(1.0252, grad_fn=<MseLossBackward0>)

Epoch 1:  50%|███████████████████████████████                               | 2/4 [01:00<01:00, 30.29s/it, loss=1.03, v_num=noih]mse_loss tensor(1.0078, grad_fn=<MseLossBackward0>)
Epoch 1:  75%|██████████████████████████████████████████████▌               | 3/4 [01:25<00:28, 28.39s/it, loss=1.03, v_num=noih]


Epoch 2:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.03, v_num=noih]mse_loss tensor(1.0056, grad_fn=<MseLossBackward0>)

Epoch 2:  25%|███████████████▌                                              | 1/4 [00:31<01:35, 31.83s/it, loss=1.03, v_num=noih]mse_loss tensor(1.0185, grad_fn=<MseLossBackward0>)

Epoch 2:  50%|███████████████████████████████                               | 2/4 [00:58<00:58, 29.32s/it, loss=1.03, v_num=noih]mse_loss tensor(1.0225, grad_fn=<MseLossBackward0>)
Epoch 2:  75%|██████████████████████████████████████████████▌               | 3/4 [01:17<00:25, 25.76s/it, loss=1.03, v_num=noih]


Epoch 3:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.03, v_num=noih]mse_loss tensor(1.0111, grad_fn=<MseLossBackward0>)

Epoch 3:  25%|███████████████▌                                              | 1/4 [00:30<01:30, 30.11s/it, loss=1.02, v_num=noih]mse_loss tensor(1.0025, grad_fn=<MseLossBackward0>)

Epoch 3:  50%|███████████████████████████████                               | 2/4 [01:04<01:04, 32.25s/it, loss=1.02, v_num=noih]mse_loss tensor(1.0221, grad_fn=<MseLossBackward0>)
Epoch 3:  75%|██████████████████████████████████████████████▌               | 3/4 [01:29<00:29, 29.80s/it, loss=1.02, v_num=noih]


Epoch 4:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.02, v_num=noih]mse_loss tensor(1.0056, grad_fn=<MseLossBackward0>)

Epoch 4:  25%|███████████████▌                                              | 1/4 [00:37<01:51, 37.13s/it, loss=1.02, v_num=noih]mse_loss tensor(1.0097, grad_fn=<MseLossBackward0>)

Epoch 4:  50%|███████████████████████████████                               | 2/4 [01:04<01:04, 32.24s/it, loss=1.02, v_num=noih]mse_loss tensor(1.0104, grad_fn=<MseLossBackward0>)
Epoch 4:  75%|██████████████████████████████████████████████▌               | 3/4 [01:22<00:27, 27.39s/it, loss=1.02, v_num=noih]


Epoch 5:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.02, v_num=noih]mse_loss tensor(0.9880, grad_fn=<MseLossBackward0>)

Epoch 5:  25%|███████████████▌                                              | 1/4 [00:32<01:36, 32.10s/it, loss=1.02, v_num=noih]mse_loss tensor(1.0004, grad_fn=<MseLossBackward0>)
Epoch 5:  50%|███████████████████████████████                               | 2/4 [00:58<00:58, 29.46s/it, loss=1.02, v_num=noih]mse_loss tensor(1.0378, grad_fn=<MseLossBackward0>)
Epoch 5:  75%|██████████████████████████████████████████████▌               | 3/4 [01:16<00:25, 25.41s/it, loss=1.02, v_num=noih]


Epoch 6:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.02, v_num=noih]mse_loss tensor(1.0074, grad_fn=<MseLossBackward0>)

Epoch 6:  25%|███████████████▌                                              | 1/4 [00:32<01:38, 32.75s/it, loss=1.02, v_num=noih]mse_loss tensor(1.0130, grad_fn=<MseLossBackward0>)
Epoch 6:  50%|███████████████████████████████                               | 2/4 [00:58<00:58, 29.26s/it, loss=1.02, v_num=noih]mse_loss tensor(0.9990, grad_fn=<MseLossBackward0>)
Epoch 6:  75%|██████████████████████████████████████████████▌               | 3/4 [01:20<00:26, 26.75s/it, loss=1.01, v_num=noih]


Epoch 7:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0116, grad_fn=<MseLossBackward0>)

Epoch 7:  25%|███████████████▌                                              | 1/4 [00:36<01:48, 36.06s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9964, grad_fn=<MseLossBackward0>)

Epoch 7:  50%|███████████████████████████████                               | 2/4 [01:12<01:12, 36.10s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0151, grad_fn=<MseLossBackward0>)
Epoch 7:  75%|██████████████████████████████████████████████▌               | 3/4 [01:33<00:31, 31.26s/it, loss=1.01, v_num=noih]


Epoch 8:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0165, grad_fn=<MseLossBackward0>)

Epoch 8:  25%|███████████████▌                                              | 1/4 [00:37<01:52, 37.54s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0065, grad_fn=<MseLossBackward0>)

Epoch 8:  50%|███████████████████████████████                               | 2/4 [01:09<01:09, 34.91s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9953, grad_fn=<MseLossBackward0>)
Epoch 8:  75%|██████████████████████████████████████████████▌               | 3/4 [01:30<00:30, 30.31s/it, loss=1.01, v_num=noih]


Epoch 9:   0%|                                                                      | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0028, grad_fn=<MseLossBackward0>)

Epoch 9:  25%|███████████████▌                                              | 1/4 [00:31<01:35, 31.75s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0122, grad_fn=<MseLossBackward0>)

Epoch 9:  50%|███████████████████████████████                               | 2/4 [01:02<01:02, 31.33s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0038, grad_fn=<MseLossBackward0>)
Epoch 9:  75%|██████████████████████████████████████████████▌               | 3/4 [01:18<00:26, 26.10s/it, loss=1.01, v_num=noih]


Epoch 10:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0062, grad_fn=<MseLossBackward0>)

Epoch 10:  25%|███████████████▎                                             | 1/4 [00:34<01:42, 34.12s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0148, grad_fn=<MseLossBackward0>)

Epoch 10:  50%|██████████████████████████████▌                              | 2/4 [01:02<01:02, 31.03s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9960, grad_fn=<MseLossBackward0>)
Epoch 10:  75%|█████████████████████████████████████████████▊               | 3/4 [01:24<00:28, 28.01s/it, loss=1.01, v_num=noih]


Epoch 11:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0050, grad_fn=<MseLossBackward0>)

Epoch 11:  25%|███████████████▎                                             | 1/4 [00:29<01:28, 29.64s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9937, grad_fn=<MseLossBackward0>)

Epoch 11:  50%|██████████████████████████████▌                              | 2/4 [00:57<00:57, 28.78s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0220, grad_fn=<MseLossBackward0>)
Epoch 11:  75%|█████████████████████████████████████████████▊               | 3/4 [01:18<00:26, 26.15s/it, loss=1.01, v_num=noih]


Epoch 12:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(0.9764, grad_fn=<MseLossBackward0>)

Epoch 12:  25%|███████████████▎                                             | 1/4 [00:30<01:30, 30.30s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0217, grad_fn=<MseLossBackward0>)

Epoch 12:  50%|██████████████████████████████▌                              | 2/4 [00:57<00:57, 28.56s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0238, grad_fn=<MseLossBackward0>)
Epoch 12:  75%|█████████████████████████████████████████████▊               | 3/4 [01:23<00:27, 28.00s/it, loss=1.01, v_num=noih]


Epoch 13:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0126, grad_fn=<MseLossBackward0>)

Epoch 13:  25%|███████████████▎                                             | 1/4 [00:37<01:53, 37.75s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0063, grad_fn=<MseLossBackward0>)

Epoch 13:  50%|██████████████████████████████▌                              | 2/4 [01:05<01:05, 32.62s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9978, grad_fn=<MseLossBackward0>)
Epoch 13:  75%|█████████████████████████████████████████████▊               | 3/4 [01:25<00:28, 28.55s/it, loss=1.01, v_num=noih]


Epoch 14:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0053, grad_fn=<MseLossBackward0>)

Epoch 14:  25%|███████████████▎                                             | 1/4 [00:31<01:33, 31.31s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0086, grad_fn=<MseLossBackward0>)

Epoch 14:  50%|██████████████████████████████▌                              | 2/4 [00:56<00:56, 28.36s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0043, grad_fn=<MseLossBackward0>)
Epoch 14:  75%|█████████████████████████████████████████████▊               | 3/4 [01:20<00:26, 26.81s/it, loss=1.01, v_num=noih]


Epoch 15:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0010, grad_fn=<MseLossBackward0>)

Epoch 15:  25%|███████████████▎                                             | 1/4 [00:31<01:34, 31.43s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0103, grad_fn=<MseLossBackward0>)

Epoch 15:  50%|██████████████████████████████▌                              | 2/4 [00:59<00:59, 29.74s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0076, grad_fn=<MseLossBackward0>)
Epoch 15:  75%|█████████████████████████████████████████████▊               | 3/4 [01:19<00:26, 26.53s/it, loss=1.01, v_num=noih]


Epoch 16:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0148, grad_fn=<MseLossBackward0>)

Epoch 16:  25%|███████████████▎                                             | 1/4 [00:29<01:29, 29.67s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0111, grad_fn=<MseLossBackward0>)

Epoch 16:  50%|██████████████████████████████▌                              | 2/4 [00:57<00:57, 28.59s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9896, grad_fn=<MseLossBackward0>)
Epoch 16:  75%|█████████████████████████████████████████████▊               | 3/4 [01:18<00:26, 26.13s/it, loss=1.01, v_num=noih]


Epoch 17:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0195, grad_fn=<MseLossBackward0>)

Epoch 17:  25%|███████████████▎                                             | 1/4 [00:31<01:35, 31.99s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9940, grad_fn=<MseLossBackward0>)

Epoch 17:  50%|██████████████████████████████▌                              | 2/4 [01:00<01:00, 30.27s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0047, grad_fn=<MseLossBackward0>)
Epoch 17:  75%|█████████████████████████████████████████████▊               | 3/4 [01:15<00:25, 25.22s/it, loss=1.01, v_num=noih]


Epoch 18:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(0.9880, grad_fn=<MseLossBackward0>)

Epoch 18:  25%|███████████████▎                                             | 1/4 [00:21<01:03, 21.05s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0104, grad_fn=<MseLossBackward0>)
Epoch 18:  50%|██████████████████████████████▌                              | 2/4 [00:39<00:39, 19.71s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0229, grad_fn=<MseLossBackward0>)
Epoch 18:  75%|█████████████████████████████████████████████▊               | 3/4 [00:48<00:16, 16.14s/it, loss=1.01, v_num=noih]


Epoch 19:   0%|                                                                     | 0/4 [00:00<?, ?it/s, loss=1.01, v_num=noih]mse_loss tensor(1.0139, grad_fn=<MseLossBackward0>)

Epoch 19:  25%|███████████████▎                                             | 1/4 [00:16<00:50, 16.71s/it, loss=1.01, v_num=noih]mse_loss tensor(1.0125, grad_fn=<MseLossBackward0>)

Epoch 19:  50%|██████████████████████████████▌                              | 2/4 [00:33<00:33, 16.98s/it, loss=1.01, v_num=noih]mse_loss tensor(0.9896, grad_fn=<MseLossBackward0>)
Epoch 19:  75%|█████████████████████████████████████████████▊               | 3/4 [00:43<00:14, 14.48s/it, loss=1.01, v_num=noih]
Validation DataLoader 0:   0%|                                                                             | 0/1 [00:00<?, ?it/s]
`Trainer.fit` stopped: `max_epochs=20` reached.
/Users/sujinchoi/anaconda3/envs/TFformer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.

Epoch 19: 100%|█████████████████████████████████████████████████████████████| 4/4 [00:47<00:00, 11.79s/it, loss=1.01, v_num=noih]
Testing DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.49s/it]
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Runningstage.testing metric      DataLoader 0
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_mse_loss         0.9743501862303612
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────